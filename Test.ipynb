{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import repository\n",
    "import configuration\n",
    "\n",
    "from repository import Repository\n",
    "from configuration import config\n",
    "repository = Repository(config)\n",
    "dataset, labels = repository.get_dataset_and_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5321100917431193"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "# Ensure that there are no NaNs\n",
    "dataset = dataset.fillna(-85)\n",
    "# Split the dataset into training (90 \\%) and testing (10 \\%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset, labels,\n",
    "test_size = 0.1 )\n",
    "\n",
    "cv = ShuffleSplit(X_train.shape[0], n_iter=10, test_size=0.2,\n",
    "random_state=0)\n",
    "\n",
    "# Define the classifier to use\n",
    "estimator = SVC(kernel='linear')\n",
    "\n",
    "# Define parameter space.\n",
    "gammas = np.logspace(-6, -1, 10)\n",
    "\n",
    "# Use Test dataset and use cross validation to find bet hyper-parameters.\n",
    "classifier = GridSearchCV(estimator=estimator, cv=cv,\n",
    "param_grid=dict(gamma=gammas))\n",
    "classifier.fit(X_train, [repository.locations.keys().index(tuple(l)) for\n",
    "l in y_train])\n",
    "\n",
    "# Test final results with the testing dataset\n",
    "classifier.score(X_test, [repository.locations.keys().index(tuple(l)) for\n",
    "l in y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([153])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print dataset.loc[1]\n",
    "tuple1 = ('00:01:8e:fe:16:6a', -52)\n",
    "tuple2 = ('00:07:40:07:63:bc', -40)\n",
    "testpredict = [tuple1, tuple2]\n",
    "#print testpredict\n",
    "\n",
    "dataset2 = dataset\n",
    "dataset2 = dataset2[dataset.index == 45]\n",
    "classifier.predict(dataset2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1455\n"
     ]
    }
   ],
   "source": [
    "from repository import Repository\n",
    "from configuration import config\n",
    "import math\n",
    "repository = Repository(config)\n",
    "dataset, labels = repository.get_dataset_and_labels()\n",
    "\n",
    "\n",
    "def diff(a, b):\n",
    "  b = set(b)\n",
    "  return [aa for aa in a if aa not in b]\n",
    "\n",
    "\n",
    "deletecolumns = []\n",
    "\n",
    "dataset = dataset.fillna(-85)\n",
    "\n",
    "#set the location labels\n",
    "numerical_labels = np.array([repository.locations.keys().index(tuple(l)) for l in labels])\n",
    "numerical_labels = map (int, numerical_labels)\n",
    "dataset['location',] = numerical_labels\n",
    "\n",
    "#first clean all AP with maxvalue of -85\n",
    "#tihs will increase the speed of the second operation\n",
    "for column in dataset:\n",
    "    datalist = dataset[column].tolist()\n",
    "    if max(datalist) <= -85:\n",
    "        deletecolumns.append(column)\n",
    "        \n",
    "dataset = dataset.drop(deletecolumns, axis = 1)\n",
    "\n",
    "# reset deletecolumns just in case\n",
    "deletecolumns = []\n",
    "\n",
    "# we have good and bad AP\n",
    "goodAP = []\n",
    "badAP = []\n",
    "\n",
    "#count amount of different locations we have\n",
    "loc_amount = len(set(numerical_labels))\n",
    "\n",
    "for location in range(0, loc_amount):\n",
    "    \n",
    "    # check only Data for current location\n",
    "    location_dataset = dataset.loc[lambda dataset: dataset.location == location, : ]\n",
    "    \n",
    "    # amount of measurement data for current location\n",
    "    measure_amount = len(dataset.loc[lambda dataset: dataset.location == location, : ])\n",
    "\n",
    "    # if a AP appears >70% in the data for a location its a good access point.\n",
    "    threshhold = math.ceil(measure_amount*0.7)\n",
    "\n",
    "    for column in location_dataset:\n",
    "        if column != \"location\":\n",
    "            datalist = location_dataset[column].tolist()\n",
    "            amount = sum(i > -85 for i in datalist)\n",
    "            \n",
    "            if amount > threshhold:\n",
    "                goodAP.append(column)\n",
    "            else:\n",
    "                badAP.append(column)\n",
    "\n",
    "badAP = set(badAP)\n",
    "goodAP = set(goodAP)\n",
    "                \n",
    "deletecolumns = diff (badAP, goodAP)\n",
    "print len(deletecolumns)\n",
    "# delete those columns\n",
    "# datasetnew = dataset.drop(deletecolumns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6540, 1840)\n(6540, 385)\n"
     ]
    }
   ],
   "source": [
    "datasetnew = dataset.drop(deletecolumns, axis = 1)\n",
    "\n",
    "print dataset.shape\n",
    "print datasetnew.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert to numerical labels\n",
    "csv = dataset.sort_values('location')\n",
    "output = csv.loc[: , ['location', '2c:36:f8:60:23:56','2c:36:f8:60:23:55']]\n",
    "output = output.replace(-85, \"-\")\n",
    "output.to_csv(\"phil-dataset.csv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}